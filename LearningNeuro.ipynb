{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningNeuro.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOEw-_O6bwRl",
        "outputId": "143ffb27-e4b1-4893-ae34-8d016046507b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resizing (Resizing)         (None, 32, 32, 1)         0         \n",
            "                                                                 \n",
            " normalization (Normalizatio  (None, 32, 32, 1)        3         \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12544)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1605760   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,624,966\n",
            "Trainable params: 1,624,963\n",
            "Non-trainable params: 3\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.9837 - accuracy: 0.4875 - val_loss: 0.9046 - val_accuracy: 0.6667\n",
            "Epoch 2/12\n",
            "4/4 [==============================] - 1s 200ms/step - loss: 0.6348 - accuracy: 0.8333 - val_loss: 0.9281 - val_accuracy: 0.8333\n",
            "Epoch 3/12\n",
            "4/4 [==============================] - 1s 198ms/step - loss: 0.4577 - accuracy: 0.8708 - val_loss: 1.0527 - val_accuracy: 0.8333\n",
            "Epoch 3: early stopping\n",
            "INFO:tensorflow:Assets written to: drive/MyDrive/Model/assets\n",
            "It`s done\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "rate = 8000\n",
        "\n",
        "\n",
        "def decode_audio(audio_binary):\n",
        "  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
        "  return tf.squeeze(audio, axis=-1)\n",
        "\n",
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  return waveform, label\n",
        "\n",
        "def get_spectrogram(waveform):\n",
        "  input_len = rate\n",
        "  waveform = waveform[:input_len]\n",
        "  zero_padding = tf.zeros(\n",
        "      [rate] - tf.shape(waveform),\n",
        "      dtype=tf.float32)\n",
        "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  spectrogram = tf.signal.stft(\n",
        "      equal_length, frame_length=255, frame_step=128)\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram\n",
        "\n",
        "def get_spectrogram_and_label_id(audio, label):\n",
        "  spectrogram = get_spectrogram(audio)\n",
        "  label_id = tf.math.argmax(label == commands)\n",
        "  return spectrogram, label_id\n",
        "\n",
        "def preprocess_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(\n",
        "      map_func=get_waveform_and_label,\n",
        "      num_parallel_calls=AUTOTUNE)\n",
        "  output_ds = output_ds.map(\n",
        "      map_func=get_spectrogram_and_label_id,\n",
        "      num_parallel_calls=AUTOTUNE)\n",
        "  return output_ds\n",
        "\n",
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(\n",
        "      input=file_path,\n",
        "      sep=os.path.sep)\n",
        "  return parts[-2]\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "#Путь к папке содержащей обучающие данные\n",
        "DATASET_PATH = 'DATASET/'\n",
        "#Путь сохранения файлов модели\n",
        "model_path = 'Model/'\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "\n",
        "#Слова для обучения\n",
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[commands != 'README.md']\n",
        "file = open('commands.txt', 'w', encoding=\"utf-8\")\n",
        "#Файл со всеми словами\n",
        "file.write(commands.numpy().tolist())\n",
        "#Все файлы для обучения\n",
        "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "filenames = tf.random.shuffle(filenames)\n",
        "num_samples = len(filenames)\n",
        "learn_percent = 0.8\n",
        "test_valid_percent = 0.1\n",
        "ration = [int(len(filenames) * learn_percent), int(len(filenames) * test_valid_percent)]\n",
        "#Выделение файлов для обучения, закрепления, тестировния (Соотношение 80:10:10)\n",
        "train_files = filenames[:ration[0]]\n",
        "val_files = filenames[ration[0]: sum(ration)]\n",
        "test_files = filenames[-ration[1]:]\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "\n",
        "waveform_ds = files_ds.map(\n",
        "    map_func=get_waveform_and_label,\n",
        "    num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "spectrogram_ds = waveform_ds.map(\n",
        "  map_func=get_spectrogram_and_label_id,\n",
        "  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)\n",
        "\n",
        "batch_size = 64\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
        "\n",
        "for spectrogram, _ in spectrogram_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "num_labels = len(commands)\n",
        "\n",
        "norm_layer = layers.Normalization()\n",
        "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    layers.Resizing(32, 32),\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "EPOCHS = 6\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")\n",
        "\n",
        "model.save(model_path)\n",
        "print('It`s done')\n"
      ]
    }
  ]
}
